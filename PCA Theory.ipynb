{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5030ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA THEORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a2f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal components analysis (PCA) is a method for finding low-dimensional representations of a data set \n",
    "#that retain as much of the original variation as possible\n",
    "\n",
    "# converts to LOw dimesnional\n",
    "\n",
    "# Logic : NOT all of these dimensions are equally interesting\n",
    "\n",
    "# The hope is to use a small subset of these linear feature combinations in further analysis\n",
    "  #while retaining most of the information present in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4a92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee105f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA examines the covariance among features and combines multiple features into a smaller set of uncorrelated variables.\n",
    "\n",
    "# These new features, which are weighted combinations of the original predictor set, are called principal components (PCs) \n",
    "  #and hopefully a small subset of them explain most of the variability of the full feature set.\n",
    "\n",
    "    \n",
    "#  primary goal in PCA is dimension reduction (in this case, feature reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bdc1199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many PC?\n",
    "\n",
    "#There are three common approaches in helping to make this decision:\n",
    "\n",
    "#Eigenvalue criterion\n",
    "#Proportion of variance explained criterion\n",
    "#Scree plot criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ac97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvalue criterion\n",
    "# The sum of the eigenvalues is equal to the number of variables entered into the PCA; \n",
    "# however, the eigenvalues will range from greater than one to near zero.\n",
    "\n",
    "# An eigenvalue of 1 means that the principal component would explain about one variableâ€™s worth of the variabili\n",
    "\n",
    "# the eigenvalue criterion states that only components with eigenvalues greater than 1 should be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4e1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of variance explained criterion\n",
    "\n",
    "#The proportion of variance explained (PVE) \n",
    "#identifies the optimal number of PCs to keep based on the total variability that we would like to account for.\n",
    "\n",
    "# choose the number of PCs required to explain at least 75% of the variability : Cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaff5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot criterion\n",
    "\n",
    "# A scree plot shows the eigenvalues or PVE for each individual PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71f143be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# explained_variance_ \n",
    "\n",
    "# explained_variance_ratio_\n",
    "\n",
    "# feature_names_in_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7db7e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination, or RFE for short, is a popular feature selection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48c8e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE is popular because it is easy to configure and use and because it is effective at selecting those features (columns) \n",
    "#in a training dataset that are more or most relevant in predicting the target variable.\n",
    "\n",
    "# features (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d0698fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This technique begins by building a model on the entire set of predictors \n",
    "#and computing an importance score for each predictor. \n",
    "#The least important predictor(s) are then removed, \n",
    "#the model is re-built,\n",
    "#and importance scores are computed again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4a680a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t random forest is used with RFE is because this model has a well-known internal method for measuring feature importance.\n",
    "\n",
    "# greedy search methods such as simple univariate filters and recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51a65942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stepwise selection was original developed as a feature selection technique for linear regression models."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fad3984",
   "metadata": {},
   "source": [
    "PCA Steps\n",
    "Standardize the data.\n",
    "Use the standardized data to create a covariance matrix.\n",
    "Use the resulting matrix to calculate eigenvectors (principal components) and their corresponding eigenvalues.\n",
    "Sort the components in decending order by its eigenvalue.\n",
    "Choose n components which explain the most variance within the data (larger eigenvalue means the feature explains more variance).\n",
    "Create a new matrix using the n components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80c7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
